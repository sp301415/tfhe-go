// Code generated by command: go run asmgen.go -vec_cmplx -out ../../math/poly/asm_vec_cmplx_amd64.s -stubs ../../math/poly/asm_vec_cmplx_stub_amd64.go -pkg=poly. DO NOT EDIT.

//go:build amd64 && !purego

#include "textflag.h"

// func addCmplxToAVX2(vOut []float64, v0 []float64, v1 []float64)
// Requires: AVX
TEXT ·addCmplxToAVX2(SB), NOSPLIT, $0-72
	MOVQ vOut_base+0(FP), AX
	MOVQ v0_base+24(FP), CX
	MOVQ v1_base+48(FP), DX
	MOVQ vOut_len+8(FP), BX
	XORQ SI, SI
	JMP  loop_end

loop_body:
	VMOVUPD (CX)(SI*8), Y0
	VMOVUPD (DX)(SI*8), Y1
	VADDPD  Y1, Y0, Y0
	VMOVUPD Y0, (AX)(SI*8)
	ADDQ    $0x04, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	RET

// func subCmplxToAVX2(vOut []float64, v0 []float64, v1 []float64)
// Requires: AVX
TEXT ·subCmplxToAVX2(SB), NOSPLIT, $0-72
	MOVQ vOut_base+0(FP), AX
	MOVQ v0_base+24(FP), CX
	MOVQ v1_base+48(FP), DX
	MOVQ vOut_len+8(FP), BX
	XORQ SI, SI
	JMP  loop_end

loop_body:
	VMOVUPD (CX)(SI*8), Y0
	VMOVUPD (DX)(SI*8), Y1
	VSUBPD  Y1, Y0, Y0
	VMOVUPD Y0, (AX)(SI*8)
	ADDQ    $0x04, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	RET

// func negCmplxToAVX2(vOut []float64, v []float64)
// Requires: AVX
TEXT ·negCmplxToAVX2(SB), NOSPLIT, $0-48
	MOVQ   vOut_base+0(FP), AX
	MOVQ   v_base+24(FP), CX
	MOVQ   vOut_len+8(FP), DX
	VXORPD Y0, Y0, Y0
	XORQ   BX, BX
	JMP    loop_end

loop_body:
	VMOVUPD (CX)(BX*8), Y1
	VSUBPD  Y1, Y0, Y1
	VMOVUPD Y1, (AX)(BX*8)
	ADDQ    $0x04, BX

loop_end:
	CMPQ BX, DX
	JL   loop_body
	RET

// func floatMulCmplxToAVX2(vOut []float64, v []float64, c float64)
// Requires: AVX
TEXT ·floatMulCmplxToAVX2(SB), NOSPLIT, $0-56
	MOVQ         vOut_base+0(FP), AX
	MOVQ         v_base+24(FP), CX
	MOVQ         vOut_len+8(FP), DX
	VBROADCASTSD c+48(FP), Y0
	XORQ         BX, BX
	JMP          loop_end

loop_body:
	VMOVUPD (CX)(BX*8), Y1
	VMULPD  Y0, Y1, Y1
	VMOVUPD Y1, (AX)(BX*8)
	ADDQ    $0x04, BX

loop_end:
	CMPQ BX, DX
	JL   loop_body
	RET

// func floatMulAddCmplxToAVX2(vOut []float64, v []float64, c float64)
// Requires: AVX, FMA3
TEXT ·floatMulAddCmplxToAVX2(SB), NOSPLIT, $0-56
	MOVQ         v_base+24(FP), AX
	MOVQ         vOut_base+0(FP), CX
	MOVQ         vOut_len+8(FP), DX
	VBROADCASTSD c+48(FP), Y0
	XORQ         BX, BX
	JMP          loop_end

loop_body:
	VMOVUPD     (AX)(BX*8), Y1
	VMOVUPD     (CX)(BX*8), Y2
	VFMADD231PD Y0, Y1, Y2
	VMOVUPD     Y2, (CX)(BX*8)
	ADDQ        $0x04, BX

loop_end:
	CMPQ BX, DX
	JL   loop_body
	RET

// func floatMulSubCmplxToAVX2(vOut []float64, v []float64, c float64)
// Requires: AVX, FMA3
TEXT ·floatMulSubCmplxToAVX2(SB), NOSPLIT, $0-56
	MOVQ         vOut_base+0(FP), AX
	MOVQ         v_base+24(FP), CX
	MOVQ         vOut_len+8(FP), DX
	VBROADCASTSD c+48(FP), Y0
	XORQ         BX, BX
	JMP          loop_end

loop_body:
	VMOVUPD      (CX)(BX*8), Y1
	VMOVUPD      (AX)(BX*8), Y2
	VFNMADD231PD Y0, Y1, Y2
	VMOVUPD      Y2, (AX)(BX*8)
	ADDQ         $0x04, BX

loop_end:
	CMPQ BX, DX
	JL   loop_body
	RET

// func cmplxMulCmplxToAVX2(vOut []float64, v []float64, c complex128)
// Requires: AVX, FMA3
TEXT ·cmplxMulCmplxToAVX2(SB), NOSPLIT, $0-64
	MOVQ         vOut_base+0(FP), AX
	MOVQ         v_base+24(FP), CX
	MOVQ         vOut_len+8(FP), DX
	VBROADCASTSD c_real+48(FP), Y0
	VBROADCASTSD c_imag+56(FP), Y1
	XORQ         BX, BX
	JMP          loop_end

loop_body:
	VMOVUPD      (CX)(BX*8), Y2
	VMOVUPD      32(CX)(BX*8), Y3
	VMULPD       Y0, Y2, Y4
	VFNMADD231PD Y1, Y3, Y4
	VMULPD       Y0, Y3, Y3
	VFMADD231PD  Y1, Y2, Y3
	VMOVUPD      Y4, (AX)(BX*8)
	VMOVUPD      Y3, 32(AX)(BX*8)
	ADDQ         $0x08, BX

loop_end:
	CMPQ BX, DX
	JL   loop_body
	RET

// func cmplxMulAddCmplxToAVX2(vOut []float64, v []float64, c complex128)
// Requires: AVX, FMA3
TEXT ·cmplxMulAddCmplxToAVX2(SB), NOSPLIT, $0-64
	MOVQ         vOut_base+0(FP), AX
	MOVQ         v_base+24(FP), CX
	MOVQ         vOut_len+8(FP), DX
	VBROADCASTSD c_real+48(FP), Y0
	VBROADCASTSD c_imag+56(FP), Y1
	XORQ         BX, BX
	JMP          loop_end

loop_body:
	VMOVUPD      (CX)(BX*8), Y2
	VMOVUPD      32(CX)(BX*8), Y3
	VMOVUPD      (AX)(BX*8), Y4
	VMOVUPD      32(AX)(BX*8), Y5
	VFMADD231PD  Y0, Y2, Y4
	VFNMADD231PD Y1, Y3, Y4
	VFMADD231PD  Y0, Y3, Y5
	VFMADD231PD  Y1, Y2, Y5
	VMOVUPD      Y4, (AX)(BX*8)
	VMOVUPD      Y5, 32(AX)(BX*8)
	ADDQ         $0x08, BX

loop_end:
	CMPQ BX, DX
	JL   loop_body
	RET

// func cmplxMulSubCmplxToAVX2(vOut []float64, v []float64, c complex128)
// Requires: AVX, FMA3
TEXT ·cmplxMulSubCmplxToAVX2(SB), NOSPLIT, $0-64
	MOVQ         vOut_base+0(FP), AX
	MOVQ         v_base+24(FP), CX
	MOVQ         vOut_len+8(FP), DX
	VBROADCASTSD c_real+48(FP), Y0
	VBROADCASTSD c_imag+56(FP), Y1
	XORQ         BX, BX
	JMP          loop_end

loop_body:
	VMOVUPD      (CX)(BX*8), Y2
	VMOVUPD      32(CX)(BX*8), Y3
	VMOVUPD      (AX)(BX*8), Y4
	VMOVUPD      32(AX)(BX*8), Y5
	VFNMADD231PD Y0, Y2, Y4
	VFMADD231PD  Y1, Y3, Y4
	VFNMADD231PD Y0, Y3, Y5
	VFNMADD231PD Y1, Y2, Y5
	VMOVUPD      Y4, (AX)(BX*8)
	VMOVUPD      Y5, 32(AX)(BX*8)
	ADDQ         $0x08, BX

loop_end:
	CMPQ BX, DX
	JL   loop_body
	RET

// func mulCmplxToAVX2(vOut []float64, v0 []float64, v1 []float64)
// Requires: AVX, FMA3
TEXT ·mulCmplxToAVX2(SB), NOSPLIT, $0-72
	MOVQ vOut_base+0(FP), AX
	MOVQ v0_base+24(FP), CX
	MOVQ v1_base+48(FP), DX
	MOVQ vOut_len+8(FP), BX
	XORQ SI, SI
	JMP  loop_end

loop_body:
	VMOVUPD      (CX)(SI*8), Y0
	VMOVUPD      32(CX)(SI*8), Y1
	VMOVUPD      (DX)(SI*8), Y2
	VMOVUPD      32(DX)(SI*8), Y3
	VMULPD       Y0, Y2, Y4
	VFNMADD231PD Y1, Y3, Y4
	VMULPD       Y0, Y3, Y0
	VFMADD231PD  Y1, Y2, Y0
	VMOVUPD      Y4, (AX)(SI*8)
	VMOVUPD      Y0, 32(AX)(SI*8)
	ADDQ         $0x08, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	RET

// func mulAddCmplxToAVX2(vOut []float64, v0 []float64, v1 []float64)
// Requires: AVX, FMA3
TEXT ·mulAddCmplxToAVX2(SB), NOSPLIT, $0-72
	MOVQ vOut_base+0(FP), AX
	MOVQ v0_base+24(FP), CX
	MOVQ v1_base+48(FP), DX
	MOVQ vOut_len+8(FP), BX
	XORQ SI, SI
	JMP  loop_end

loop_body:
	VMOVUPD      (CX)(SI*8), Y0
	VMOVUPD      32(CX)(SI*8), Y1
	VMOVUPD      (DX)(SI*8), Y2
	VMOVUPD      32(DX)(SI*8), Y3
	VMOVUPD      (AX)(SI*8), Y4
	VMOVUPD      32(AX)(SI*8), Y5
	VFMADD231PD  Y0, Y2, Y4
	VFNMADD231PD Y1, Y3, Y4
	VFMADD231PD  Y0, Y3, Y5
	VFMADD231PD  Y1, Y2, Y5
	VMOVUPD      Y4, (AX)(SI*8)
	VMOVUPD      Y5, 32(AX)(SI*8)
	ADDQ         $0x08, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	RET

// func mulSubCmplxToAVX2(vOut []float64, v0 []float64, v1 []float64)
// Requires: AVX, FMA3
TEXT ·mulSubCmplxToAVX2(SB), NOSPLIT, $0-72
	MOVQ vOut_base+0(FP), AX
	MOVQ v0_base+24(FP), CX
	MOVQ v1_base+48(FP), DX
	MOVQ vOut_len+8(FP), BX
	XORQ SI, SI
	JMP  loop_end

loop_body:
	VMOVUPD      (CX)(SI*8), Y0
	VMOVUPD      32(CX)(SI*8), Y1
	VMOVUPD      (DX)(SI*8), Y2
	VMOVUPD      32(DX)(SI*8), Y3
	VMOVUPD      (AX)(SI*8), Y4
	VMOVUPD      32(AX)(SI*8), Y5
	VFNMADD231PD Y0, Y2, Y4
	VFMADD231PD  Y1, Y3, Y4
	VFNMADD231PD Y0, Y3, Y5
	VFNMADD231PD Y1, Y2, Y5
	VMOVUPD      Y4, (AX)(SI*8)
	VMOVUPD      Y5, 32(AX)(SI*8)
	ADDQ         $0x08, SI

loop_end:
	CMPQ SI, BX
	JL   loop_body
	RET
