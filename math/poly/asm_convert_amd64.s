//go:build amd64 && !purego

#include "textflag.h"

// Magic numbers for conversion
DATA EXP52<>+0(SB)/8, $0x4330000000000000 // 2^52
GLOBL EXP52<>(SB), NOPTR | RODATA, $8
DATA EXP8463<>+0(SB)/8, $0x4530000080000000 // 2^84 + 2^63
GLOBL EXP8463<>(SB), NOPTR | RODATA, $8
DATA EXP846352<>+0(SB)/8, $0x4530000080100000 // 2^84 + 2^63 + 2^52
GLOBL EXP846352<>+0(SB), NOPTR | RODATA, $8

DATA MANT_MASK<>+0(SB)/8, $0xFFFFFFFFFFFFF // 2^52 - 1
GLOBL MANT_MASK<>+0(SB), NOPTR | RODATA, $8
DATA BIT_MANT_MASK<>+0(SB)/8, $0x10000000000000 // 2^52
GLOBL BIT_MANT_MASK<>+0(SB), NOPTR | RODATA, $8
DATA EXP_MASK<>+0(SB)/8, $0x7FF // 2^11 - 1
GLOBL EXP_MASK<>+0(SB), NOPTR | RODATA, $8
DATA EXP_SHIFT<>+0(SB)/8, $1086 // 1023 + 64 + 11
GLOBL EXP_SHIFT<>+0(SB), NOPTR | RODATA, $8

TEXT ·convertPolyToFourierPolyAssignUint32AVX2(SB), NOSPLIT, $0-48
	MOVQ p_base+0(FP), AX
	MOVQ fpOut_base+24(FP), CX

	MOVQ p_len+8(FP), DX

	MOVQ AX, BX
	SHLQ $1, DX // Offset: (N/2) * 4bytes = 2N bytes
	ADDQ DX, BX
	SHRQ $1, DX

	XORQ SI, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVDQU (AX)(DI*4), X0
	VMOVDQU (BX)(DI*4), X1

	VCVTDQ2PD X0, Y0
	VCVTDQ2PD X1, Y1

	VMOVUPD Y0, (CX)(SI*8)
	VMOVUPD Y1, 32(CX)(SI*8)

	ADDQ $8, SI
	ADDQ $4, DI

loop_end:
	CMPQ SI, DX
	JL   loop_body

	RET

TEXT ·convertPolyToFourierPolyAssignUint64AVX2(SB), NOSPLIT, $0-48
	MOVQ p_base+0(FP), AX
	MOVQ fpOut_base+24(FP), CX

	MOVQ p_len+8(FP), DX

	MOVQ AX, BX
	SHLQ $2, DX // Offset: (N/2) * 8bytes = 4N bytes
	ADDQ DX, BX
	SHRQ $2, DX

	VBROADCASTSD EXP52<>+0(SB), Y10     // 2^52
	VBROADCASTSD EXP8463<>+0(SB), Y11   // 2^84 + 2^63
	VBROADCASTSD EXP846352<>+0(SB), Y12 // 2^84 + 2^63 + 2^52

	XORQ SI, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVDQU (AX)(DI*8), Y0
	VMOVDQU (BX)(DI*8), Y1

	VPBLENDD $0b01010101, Y0, Y10, Y2
	VPSRLQ   $32, Y0, Y3
	VPXOR    Y3, Y11, Y3
	VSUBPD   Y12, Y3, Y3
	VADDPD   Y3, Y2, Y3

	VPBLENDD $0b01010101, Y1, Y10, Y4
	VPSRLQ   $32, Y1, Y5
	VPXOR    Y5, Y11, Y5
	VSUBPD   Y12, Y5, Y5
	VADDPD   Y5, Y4, Y5

	VMOVUPD Y3, (CX)(SI*8)
	VMOVUPD Y5, 32(CX)(SI*8)

	ADDQ $8, SI
	ADDQ $4, DI

loop_end:
	CMPQ SI, DX
	JL   loop_body

	RET

TEXT ·floatModQInPlaceAVX2(SB), NOSPLIT, $0-40
	MOVQ coeffs_base+0(FP), AX

	MOVQ coeffs_len+8(FP), DX

	VBROADCASTSD Q+24(FP), Y10
	VBROADCASTSD QInv+32(FP), Y11

	XORQ SI, SI
	JMP  loop_end

loop_body:
	VMOVUPD (AX)(SI*8), Y0

	VMULPD   Y11, Y0, Y0
	VROUNDPD $0, Y0, Y1
	VSUBPD   Y1, Y0, Y1
	VMULPD   Y10, Y1, Y1
	VROUNDPD $0, Y1, Y2

	VMOVUPD Y2, (AX)(SI*8)

	ADDQ $4, SI

loop_end:
	CMPQ SI, DX
	JL   loop_body

	RET

TEXT ·convertFourierPolyToPolyAssignUint32AVX2(SB), NOSPLIT, $0-48
	MOVQ fp_base+0(FP), AX
	MOVQ pOut_base+24(FP), BX

	MOVQ fp_len+8(FP), DX

	MOVQ BX, CX
	SHLQ $1, DX // Offset: (N/2) * 4bytes = 2N bytes
	ADDQ DX, CX
	SHRQ $1, DX

	XORQ SI, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVUPD (AX)(SI*8), Y0
	VMOVUPD 32(AX)(SI*8), Y1

	VCVTTPD2DQY Y0, X0
	VCVTTPD2DQY Y1, X1

	VMOVDQU X0, (BX)(DI*4)
	VMOVDQU X1, (CX)(DI*4)

	ADDQ $8, SI
	ADDQ $4, DI

loop_end:
	CMPQ SI, DX
	JL   loop_body

	RET

TEXT ·convertFourierPolyToPolyAssignUint64AVX2(SB), NOSPLIT, $0-48
	MOVQ fp_base+0(FP), AX
	MOVQ pOut_base+24(FP), BX

	MOVQ fp_len+8(FP), DX

	MOVQ BX, CX
	SHLQ $2, DX // Offset: (N/2) * 8bytes = 4N bytes
	ADDQ DX, CX
	SHRQ $2, DX

	VBROADCASTSD MANT_MASK<>+0(SB), Y10
	VBROADCASTSD BIT_MANT_MASK<>+0(SB), Y11
	VBROADCASTSD EXP_MASK<>+0(SB), Y12
	VBROADCASTSD EXP_SHIFT<>+0(SB), Y13
	VPXOR        Y14, Y14, Y14

	XORQ SI, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVUPD (AX)(SI*8), Y0
	VMOVUPD 32(AX)(SI*8), Y1

	VANDPD Y0, Y10, Y2
	VORPD  Y2, Y11, Y2 // mantissa
	VPSRLQ $52, Y0, Y3
	VANDPD Y3, Y12, Y3 // exp
	VPSRLQ $63, Y0, Y4
	VPSUBQ Y4, Y14, Y4 // sign

	VPSLLQ  $11, Y2, Y2
	VPSUBQ  Y3, Y13, Y3
	VPSRLVQ Y3, Y2, Y2
	VPSUBQ  Y2, Y14, Y3

	VPBLENDVB Y4, Y3, Y2, Y5

	VANDPD Y1, Y10, Y6
	VORPD  Y6, Y11, Y6 // mantissa
	VPSRLQ $52, Y1, Y7
	VANDPD Y7, Y12, Y7 // exp
	VPSRLQ $63, Y1, Y8
	VPSUBQ Y8, Y14, Y8 // sign

	VPSLLQ  $11, Y6, Y6
	VPSUBQ  Y7, Y13, Y7
	VPSRLVQ Y7, Y6, Y6
	VPSUBQ  Y6, Y14, Y7

	VPBLENDVB Y8, Y7, Y6, Y9

	VMOVDQU Y5, (BX)(DI*8)
	VMOVDQU Y9, (CX)(DI*8)

	ADDQ $8, SI
	ADDQ $4, DI

loop_end:
	CMPQ SI, DX
	JL   loop_body

	RET

TEXT ·convertFourierPolyToPolyAddAssignUint32AVX2(SB), NOSPLIT, $0-48
	MOVQ fp_base+0(FP), AX
	MOVQ pOut_base+24(FP), BX

	MOVQ fp_len+8(FP), DX

	MOVQ BX, CX
	SHLQ $1, DX // Offset: (N/2) * 4bytes = 2N bytes
	ADDQ DX, CX
	SHRQ $1, DX

	XORQ SI, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVUPD (AX)(SI*8), Y0
	VMOVUPD 32(AX)(SI*8), Y1
	VMOVUPD (BX)(DI*4), X2
	VMOVUPD (CX)(DI*4), X3

	VCVTPD2DQY Y0, X0
	VCVTPD2DQY Y1, X1

	VPADDD X0, X2, X0
	VPADDD X1, X3, X1

	VMOVDQU X0, (BX)(DI*4)
	VMOVDQU X1, (CX)(DI*4)

	ADDQ $8, SI
	ADDQ $4, DI

loop_end:
	CMPQ SI, DX
	JL   loop_body

	RET

TEXT ·convertFourierPolyToPolyAddAssignUint64AVX2(SB), NOSPLIT, $0-48
	MOVQ fp_base+0(FP), AX
	MOVQ pOut_base+24(FP), BX

	MOVQ fp_len+8(FP), DX

	MOVQ BX, CX
	SHLQ $2, DX // Offset: (N/2) * 8bytes = 4N bytes
	ADDQ DX, CX
	SHRQ $2, DX

	VBROADCASTSD MANT_MASK<>+0(SB), Y10
	VBROADCASTSD BIT_MANT_MASK<>+0(SB), Y11
	VBROADCASTSD EXP_MASK<>+0(SB), Y12
	VBROADCASTSD EXP_SHIFT<>+0(SB), Y13
	VPXOR        Y14, Y14, Y14

	XORQ SI, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVUPD (AX)(SI*8), Y0
	VMOVUPD 32(AX)(SI*8), Y1
	VMOVUPD (BX)(DI*8), Y5
	VMOVUPD (CX)(DI*8), Y9

	VANDPD Y0, Y10, Y2
	VORPD  Y2, Y11, Y2 // mantissa
	VPSRLQ $52, Y0, Y3
	VANDPD Y3, Y12, Y3 // exp
	VPSRLQ $63, Y0, Y4
	VPSUBQ Y4, Y14, Y4 // sign

	VPSLLQ  $11, Y2, Y2
	VPSUBQ  Y3, Y13, Y3
	VPSRLVQ Y3, Y2, Y2
	VPSUBQ  Y2, Y14, Y3

	VPBLENDVB Y4, Y3, Y2, Y4
	VPADDQ    Y4, Y5, Y5

	VANDPD Y1, Y10, Y6
	VORPD  Y6, Y11, Y6 // mantissa
	VPSRLQ $52, Y1, Y7
	VANDPD Y7, Y12, Y7 // exp
	VPSRLQ $63, Y1, Y8
	VPSUBQ Y8, Y14, Y8 // sign

	VPSLLQ  $11, Y6, Y6
	VPSUBQ  Y7, Y13, Y7
	VPSRLVQ Y7, Y6, Y6
	VPSUBQ  Y6, Y14, Y7

	VPBLENDVB Y8, Y7, Y6, Y8
	VPADDQ    Y8, Y9, Y9

	VMOVDQU Y5, (BX)(DI*8)
	VMOVDQU Y9, (CX)(DI*8)

	ADDQ $8, SI
	ADDQ $4, DI

loop_end:
	CMPQ SI, DX
	JL   loop_body

	RET

TEXT ·convertFourierPolyToPolySubAssignUint32AVX2(SB), NOSPLIT, $0-48
	MOVQ fp_base+0(FP), AX
	MOVQ pOut_base+24(FP), BX

	MOVQ fp_len+8(FP), DX

	MOVQ BX, CX
	SHLQ $1, DX // Offset: (N/2) * 4bytes = 2N bytes
	ADDQ DX, CX
	SHRQ $1, DX

	XORQ SI, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVUPD (AX)(SI*8), Y0
	VMOVUPD 32(AX)(SI*8), Y1
	VMOVUPD (BX)(DI*4), X2
	VMOVUPD (CX)(DI*4), X3

	VCVTTPD2DQY Y0, X0
	VCVTTPD2DQY Y1, X1

	VPSUBD X0, X2, X0
	VPSUBD X1, X3, X1

	VMOVDQU X0, (BX)(DI*4)
	VMOVDQU X1, (CX)(DI*4)

	ADDQ $8, SI
	ADDQ $4, DI

loop_end:
	CMPQ SI, DX
	JL   loop_body

	RET

TEXT ·convertFourierPolyToPolySubAssignUint64AVX2(SB), NOSPLIT, $0-48
	MOVQ fp_base+0(FP), AX
	MOVQ pOut_base+24(FP), BX

	MOVQ fp_len+8(FP), DX

	MOVQ BX, CX
	SHLQ $2, DX // Offset: (N/2) * 8bytes = 4N bytes
	ADDQ DX, CX
	SHRQ $2, DX

	VBROADCASTSD MANT_MASK<>+0(SB), Y10
	VBROADCASTSD BIT_MANT_MASK<>+0(SB), Y11
	VBROADCASTSD EXP_MASK<>+0(SB), Y12
	VBROADCASTSD EXP_SHIFT<>+0(SB), Y13
	VPXOR        Y14, Y14, Y14

	XORQ SI, SI
	XORQ DI, DI
	JMP  loop_end

loop_body:
	VMOVUPD (AX)(SI*8), Y0
	VMOVUPD 32(AX)(SI*8), Y1
	VMOVUPD (BX)(DI*8), Y5
	VMOVUPD (CX)(DI*8), Y9

	VANDPD Y0, Y10, Y2
	VORPD  Y2, Y11, Y2 // mantissa
	VPSRLQ $52, Y0, Y3
	VANDPD Y3, Y12, Y3 // exp
	VPSRLQ $63, Y0, Y4
	VPSUBQ Y4, Y14, Y4 // sign

	VPSLLQ  $11, Y2, Y2
	VPSUBQ  Y3, Y13, Y3
	VPSRLVQ Y3, Y2, Y2
	VPSUBQ  Y2, Y14, Y3

	VPBLENDVB Y4, Y3, Y2, Y4
	VPSUBQ    Y4, Y5, Y5

	VANDPD Y1, Y10, Y6
	VORPD  Y6, Y11, Y6 // mantissa
	VPSRLQ $52, Y1, Y7
	VANDPD Y7, Y12, Y7 // exp
	VPSRLQ $63, Y1, Y8
	VPSUBQ Y8, Y14, Y8 // sign

	VPSLLQ  $11, Y6, Y6
	VPSUBQ  Y7, Y13, Y7
	VPSRLVQ Y7, Y6, Y6
	VPSUBQ  Y6, Y14, Y7

	VPBLENDVB Y8, Y7, Y6, Y8
	VPSUBQ    Y8, Y9, Y9

	VMOVDQU Y5, (BX)(DI*8)
	VMOVDQU Y9, (CX)(DI*8)

	ADDQ $8, SI
	ADDQ $4, DI

loop_end:
	CMPQ SI, DX
	JL   loop_body

	RET
